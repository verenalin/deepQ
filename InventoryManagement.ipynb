{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verenalin/deepQ/blob/master/InventoryManagement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT5eVdI2Bgx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJFaihAw4qoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jun 24 15:59:24 2019\n",
        "\n",
        "@author: Verena\n",
        "\"\"\"\n",
        "\n",
        "# Env#\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random \n",
        "\n",
        "class env():\n",
        "    \n",
        "    def __init__(self, mode, debug, station_history):\n",
        "        \n",
        "        print(\"Creating A inventory Environment...\")\n",
        "        \n",
        "        self.mode = mode\n",
        "        self.seed = random.randint(0, 10)\n",
        "        self.num_days = 29\n",
        "        self.current_day = 0\n",
        "        self.inventory_stock_sim = self.generate_stock(mode)\n",
        "        self.inventory_stock = self.inventory_stock_sim.copy() # Zurücksetzen für jede Episode\n",
        "        self.old_stock = self.inventory_stock[0]\n",
        "        self.new_stock = 0\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        self.inventory_moved = 0\n",
        "        self.debug = debug\n",
        "        self.actions = [0, 5, 10, 15] \n",
        "        self.n_actions = len(self.actions)\n",
        "        \n",
        "        #Features: day, old stock, new stock\n",
        "        self.n_features = 1\n",
        "        self.game_over = False\n",
        "\n",
        "        \n",
        "        if self.debug == True:\n",
        "            print(\"Generating inventory Stock: {}\".format(self.mode))\n",
        "            print(\"inventory Stock: {}\".format(self.inventory_stock))\n",
        "        \n",
        "    def generate_stock(self, mode): \n",
        "        \n",
        "        # Liste mit 30 täglichen Inventar-Werten\n",
        "        # mode: linear oder random\n",
        "        \n",
        "        inventory_stock = [20]\n",
        "        \n",
        "        if mode == \"linear\": #Inventar nimmt täglich linear um 3 Stück ab.\n",
        "            for i in range(1, 30):\n",
        "                inventory_stock.append(inventory_stock[i-1] - 3) \n",
        "             \n",
        "            \n",
        "        if mode == \"random\": #Inventar nimmt täglich um 3 Stück +- 5 ab. Bei positiven Werten handelt es sich um Retoure\n",
        "            for i in range(1, 30):\n",
        "                inventory_stock.append(inventory_stock[i-1] - 3 + np.random.random_integers(-5, 5))\n",
        "            \n",
        "        return inventory_stock\n",
        "    \n",
        "    \n",
        "    def ping_dqn(self, index): #Informationen für DQN\n",
        "        action = self.actions[index]\n",
        "        self.update_stock(action)\n",
        "        \n",
        "        if self.inventory_stock[self.current_day] > 50: \n",
        "          #Wenn Inventar > Limit, Reward = -30 (Strafzahlung) - 0.2*Inventar(Holding Cost) -0.5*Bestellmenge (Transportkosten p.u.) - 2(Fixe Bestellkosten)\n",
        "          self.reward = -30 - 0.2*self.inventory_stock[self.current_day] - 2\n",
        "         \n",
        "        elif self.inventory_stock[self.current_day] < 0:\n",
        "          #Wenn Inventar < 0, Reward = -30 (Strafzahlung) - 0.2*Inventar(Holding Cost) -0.5*Bestellmenge (Transportkosten p.u.) - 2(Fixe Bestellkosten)\n",
        "          self.reward = -30 - 0.2*self.inventory_stock[self.current_day] - 2\n",
        "        \n",
        "        else:\n",
        "          self.reward = - 0.2*self.inventory_stock[self.current_day] - 2\n",
        "          \n",
        "\n",
        "        if self.current_day != 29:\n",
        "            self.update_day()\n",
        "            self.old_stock = self.inventory_stock[self.current_day - 1]\n",
        "            self.new_stock = self.inventory_stock[self.current_day]\n",
        "            \n",
        "        if self.current_day == 29:\n",
        "            self.done = True\n",
        "          \n",
        "        return self.current_day, self.old_stock, self.new_stock, self.reward, self.done\n",
        "\n",
        "     \n",
        "    def get_old_stock(self):\n",
        "        \n",
        "        return self.old_stock\n",
        "    \n",
        "    def update_stock(self, num_inventory):\n",
        "        \n",
        "        # update Inventar basierend auf Aktion in t\n",
        "        if self.current_day != 30:\n",
        "            for day in range(self.current_day+1, len(self.inventory_stock)):\n",
        "                self.inventory_stock[day] += num_inventory #Inventar, Nachfrage bereits abgezogen, + Anzahl Orders\n",
        "                #if day < len(self.inventory_stock)-1:\n",
        "                    #self.exp_inventory_stock[day] += num_inventory\n",
        "                \n",
        "            self.inventory_moved = num_inventory #Anzahl Ordering\n",
        "        \n",
        "        else:\n",
        "            if self.debug == True:\n",
        "                print(\"Last day. Cannot Move inventorys.\")\n",
        "            pass\n",
        "        \n",
        "        return\n",
        "    \n",
        "    def update_day(self):\n",
        "        \n",
        "        # update current_day \n",
        "        self.current_day += 1\n",
        "        \n",
        "        if self.debug == True:\n",
        "            print(\"Tick... Forwarded Current day\")\n",
        "                \n",
        "        return\n",
        "    \n",
        "    def reset(self):\n",
        "        \n",
        "        if self.debug == True:\n",
        "            print(\"Reset Environment ...\")\n",
        "        \n",
        "        self.num_days = 29\n",
        "        self.current_day = 0\n",
        "        self.inventory_stock = self.inventory_stock_sim.copy()\n",
        "       # self.exp_inventory_stock = self.exp_inventory_stock_sim.copy()\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        self.inventory_moved = 0\n",
        "        self.old_stock = self.inventory_stock[0]\n",
        "        self.new_stock = 0\n",
        "        #self.expected_stock = self.exp_inventory_stock[0]\n",
        "        #self.expected_stock_new = 0\n",
        "        #return (self.current_day, self.old_stock, self.new_stock)\n",
        "        \n",
        "    def current_stock(self):\n",
        "        \n",
        "        return self.inventory_stock[self.current_day]\n",
        "    \n",
        "    def get_sim_stock(self):\n",
        "        \n",
        "        return self.inventory_stock"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktZiFBqtCQzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper, Eingabe für Form der Nachfrage #\n",
        "\n",
        "def user_input():\n",
        "    \n",
        "    '''\n",
        "    This function creates all initial parameter for the training based on \n",
        "    user inputs.\n",
        "    '''\n",
        "    \n",
        "    episode_list = [eps for eps in range(100, 250, 100)]\n",
        "    \n",
        "    # ---- inventory Stock Parameters ----\n",
        "    # linear: a linear increasing inventory stock with 3 additional inventorys per day\n",
        "    # random: a linear increasing inventory stock with random fluctuation\n",
        "    # actual_1: randomly pick traffic from one citiinventory stations\n",
        "    # -------------------------------\n",
        "    \n",
        "    data = input(\"Linear or Random?: \").lower()\n",
        "    \n",
        "    brain = input(\"Enter agent type (dqn): \").lower()\n",
        "    \n",
        "    model_based = None\n",
        "    \n",
        "    if brain == 'dqn':\n",
        "        model_based = False #kann hier if gelöscht werden?\n",
        "    \n",
        "    #if data == 'actual':\n",
        "       # station_history = citi_data_processing\n",
        "       # print(station_history)\n",
        "        \n",
        "   # else:\n",
        "    station_history = None\n",
        "    \n",
        "    return episode_list, data, brain, model_based, station_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTC6AP3x45vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class agent():\n",
        "    \n",
        "    \n",
        "    def __init__(self, epsilon, lr, gamma, current_stock, debug, model_based):\n",
        "        \n",
        "        print(\"Created an Agent ...\")\n",
        "        self.actions = [0, 5, 10, 15]\n",
        "        self.reward = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.debug = debug\n",
        "        self.current_stock = current_stock\n",
        "        self.model_based = model_based\n",
        "        \n",
        "        # performance metric\n",
        "        self.q_table = pd.DataFrame(columns = self.actions, dtype = np.float64)\n",
        "        self.monthly_action_history = []\n",
        "        self.monthly_stock_history = []\n",
        "       \n",
        "    def choose_action(self, s, ex):\n",
        "        \n",
        "        '''\n",
        "        This funciton choose an action based on Q Table. It also does \n",
        "        validation to ensure stock will not be negative after moving inventorys.\n",
        "        Input: \n",
        "            - s: current inventory\n",
        "            - ex: expected inventory stock in subsequent day (based on random forests prediction)\n",
        "        \n",
        "        Output:\n",
        "            - action: number of inventorys to move\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        self.check_state_exist(s)\n",
        "        self.current_stock = s\n",
        "        #self.expected_stock = ex\n",
        "        \n",
        "        # find valid action based on current stock \n",
        "        #valid_state_action = self.find_valid_action(self.q_table.loc[s, :])\n",
        "        if self.model_based == True:\n",
        "            self.check_state_exist(s)\n",
        "            valid_state_action = self.q_table.loc[s, :]\n",
        "\n",
        "        elif self.model_based == False:\n",
        "            valid_state_action = self.q_table.loc[s, :]\n",
        "                \n",
        "        if np.random.uniform() < self.epsilon:\n",
        "                        \n",
        "            try:\n",
        "                # find the action with the highest expected reward\n",
        "                \n",
        "                valid_state_action = valid_state_action.reindex(np.random.permutation(valid_state_action.index))\n",
        "                action = valid_state_action.idxmax()\n",
        "            \n",
        "            except:\n",
        "                # if action list is null, default to 0\n",
        "                action = 0\n",
        "                        \n",
        "            if self.debug == True:\n",
        "                print(\"Decided to Move: {}\".format(action))\n",
        "                        \n",
        "        else:\n",
        "            \n",
        "            # randomly choose an action\n",
        "            # re-pick if the action leads to negative stock\n",
        "            try:\n",
        "                action = np.random.choice(valid_state_action.index)\n",
        "            except:\n",
        "                action = 0\n",
        "            \n",
        "            if self.debug == True:\n",
        "                print(\"Randomly Move: {}\".format(action))\n",
        "        \n",
        "        self.monthly_action_history.append(action)\n",
        "        self.monthly_stock_history.append(s)\n",
        "        \n",
        "        return action\n",
        " \n",
        "    \n",
        "\n",
        "    def learn(self, s, a, r, s_, ex, g):\n",
        "\n",
        "        \n",
        "        '''\n",
        "        This function updates Q tables after each interaction with the\n",
        "        environment.\n",
        "        Input: \n",
        "            - s: current inventory stock\n",
        "            #- ex: expected inventory stock in next day\n",
        "            - a: current action (number of inventorys to move)\n",
        "            - r: reward received from current state\n",
        "            - s_: new inventory stock based on inventory moved and new stock\n",
        "        Output: None\n",
        "        '''\n",
        "        \n",
        "        if self.debug == True:\n",
        "            print(\"Moved inventorys: {}\".format(a))\n",
        "            print(\"Old inventory Stock: {}\".format(s))\n",
        "            print(\"New inventory Stock: {}\".format(s_))\n",
        "            print(\"---\")\n",
        "        \n",
        "        self.check_state_exist(s_)\n",
        "\n",
        "        if self.model_based == False:\n",
        "            q_predict = self.q_table.loc[s, a]\n",
        "        elif self.model_based == True:\n",
        "            avg = s\n",
        "            self.check_state_exist(s)\n",
        "            q_predict = self.q_table.loc[s, a]\n",
        "        \n",
        "\n",
        "        if g == False:\n",
        "            \n",
        "\n",
        "            # Updated Q Target Value if it is not end of day  \n",
        "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
        "        \n",
        "        else:\n",
        "            # Update Q Target Value as Immediate reward if end of day\n",
        "            q_target = r\n",
        "\n",
        "        if self.model_based == False:\n",
        "            self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
        "        elif self.model_based == True:\n",
        "            self.q_table.loc[avg, a] += self.lr * (q_target - q_predict)\n",
        "        \n",
        "        return\n",
        "\n",
        "    \n",
        "    def check_state_exist(self, state):\n",
        "        \n",
        "        # Add a new row with state value as index if not exist\n",
        "        \n",
        "        if state not in self.q_table.index:\n",
        "            \n",
        "            self.q_table = self.q_table.append(\n",
        "                pd.Series(\n",
        "                        [0]*len(self.actions), \n",
        "                        index = self.q_table.columns,\n",
        "                        name = state\n",
        "                        )\n",
        "                )\n",
        "        \n",
        "        return\n",
        "    \n",
        "\n",
        "    def find_valid_action(self, state_action):\n",
        "        \n",
        "        '''\n",
        "        This function check the validity acitons in a given state.\n",
        "        Input: \n",
        "            - state_action: the current state under consideration\n",
        "        Output:\n",
        "            - state_action: a pandas Series with only the valid actions that\n",
        "                            will not cause negative stock\n",
        "        '''\n",
        "        \n",
        "        # remove action that will stock to be negative\n",
        "        \n",
        "        for action in self.actions:\n",
        "            if self.current_stock + action < 0:\n",
        "                \n",
        "                if self.debug == True:\n",
        "                    print(\"Drop action {}, current stock {}\".format(action, self.current_stock))\n",
        "                \n",
        "                state_action.drop(index = action, inplace = True)\n",
        "        \n",
        "        return state_action\n",
        "        \n",
        "    \n",
        "    def print_q_table(self):\n",
        "        \n",
        "        print(self.q_table)\n",
        "\n",
        "\n",
        "    def get_q_table(self):\n",
        "        \n",
        "        return self.q_table\n",
        "\n",
        "    \n",
        "    def get_monthly_actions(self):\n",
        "        \n",
        "        return self.monthly_action_history\n",
        "    \n",
        "    def get_monthly_stocks(self):\n",
        "        \n",
        "        return self.monthly_stock_history\n",
        "\n",
        "    \n",
        "    def reset_monthly_history(self):\n",
        "        \n",
        "        self.monthly_action_history = []\n",
        "        self.monthly_stock_history = []\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYobPnONBaQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DQN#\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "\n",
        "class DeepQNetwork:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_actions,\n",
        "        n_features,\n",
        "        learning_rate=0.01,\n",
        "        reward_decay=0.9,\n",
        "        e_greedy=0.9,\n",
        "        replace_target_iter=10,\n",
        "        batch_size=5,\n",
        "        e_greedy_increment=None,\n",
        "        output_graph=None\n",
        "        \n",
        "    ):\n",
        "        self.n_actions = n_actions\n",
        "        self.n_features = n_features\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = reward_decay\n",
        "        self.epsilon_max = e_greedy\n",
        "        self.replace_target_iter = replace_target_iter\n",
        "        self.memory_size = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon_increment = e_greedy_increment\n",
        "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
        "        self.monthly_stock_history = []\n",
        "        \n",
        "        # total learning step\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # initialize zero memory [s, a, r, s_]\n",
        "        self.memory = np.zeros((self.memory_size, n_features*2+2))\n",
        "\n",
        "        # consist of [target_net, evaluate_net]\n",
        "        self._build_net()\n",
        "\n",
        "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
        "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')\n",
        "\n",
        "        with tf.variable_scope('soft_replacement'):\n",
        "            self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
        "\n",
        "        self.sess = tf.Session()\n",
        "\n",
        "        #if output_graph:\n",
        "            # $ tensorboard --logdir=logs\n",
        "         #   tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.cost_his = []\n",
        "        \n",
        "        \n",
        "    #changed tf.,,.reulu to sigmoid\n",
        "    def _build_net(self):\n",
        "        # ------------------ all inputs ------------------------\n",
        "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input State\n",
        "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')  # input Next State\n",
        "        self.r = tf.placeholder(tf.float32, [None, ], name='r')  # input Reward\n",
        "        self.a = tf.placeholder(tf.int32, [None, ], name='a')  # input Action\n",
        "\n",
        "        w_initializer, b_initializer = tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
        "\n",
        "        # ------------------ build evaluate_net ------------------\n",
        "        with tf.variable_scope('eval_net', reuse=tf.AUTO_REUSE):\n",
        "            e1 = tf.layers.dense(self.s, 20, tf.sigmoid, kernel_initializer=w_initializer,\n",
        "                                 bias_initializer=b_initializer, name='e1')\n",
        "            self.q_eval = tf.layers.dense(e1, self.n_actions, kernel_initializer=w_initializer,\n",
        "                                          bias_initializer=b_initializer, name='q')\n",
        "\n",
        "        # ------------------ build target_net ------------------\n",
        "        with tf.variable_scope('target_net', reuse=tf.AUTO_REUSE):\n",
        "            t1 = tf.layers.dense(self.s_, 20, tf.sigmoid, kernel_initializer=w_initializer,\n",
        "                                 bias_initializer=b_initializer, name='t1')\n",
        "            self.q_next = tf.layers.dense(t1, self.n_actions, kernel_initializer=w_initializer,\n",
        "                                          bias_initializer=b_initializer, name='t2')\n",
        "\n",
        "        with tf.variable_scope('q_target'):\n",
        "            q_target = self.r + self.gamma * tf.reduce_max(self.q_next, axis=1, name='Qmax_s_')    # shape=(None, )\n",
        "            self.q_target = tf.stop_gradient(q_target)\n",
        "        with tf.variable_scope('q_eval'):\n",
        "            a_indices = tf.stack([tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)\n",
        "            self.q_eval_wrt_a = tf.gather_nd(params=self.q_eval, indices=a_indices)    # shape=(None, )\n",
        "        with tf.variable_scope('loss'):\n",
        "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_wrt_a, name='TD_error'))\n",
        "        with tf.variable_scope('train', reuse=tf.AUTO_REUSE):\n",
        "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
        "    \n",
        "    def store_transition(self, s, a, r, s_):\n",
        "        if not hasattr(self, 'memory_counter'):\n",
        "            self.memory_counter = 0\n",
        "        transition = np.hstack((s, [a, r], s_))\n",
        "        # replace the old memory with new memory\n",
        "        index = self.memory_counter % self.memory_size\n",
        "        self.memory[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "    \n",
        "    def choose_action(self, observation):\n",
        "        # to have batch dimension when feed into tf placeholder\n",
        "        #observation = observation[np.newaxis, :]\n",
        "        #print(\"hello\")\n",
        "        self.monthly_stock_history.append(observation)\n",
        "        observation = np.array([[observation]])\n",
        "\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            # forward feed the observation and get q value for every actions\n",
        "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
        "            #print(\">>>>>> action value: {} <<<<<<\".format(actions_value))\n",
        "            action = np.argmax(actions_value)\n",
        "        else:\n",
        "            action = np.random.randint(0, self.n_actions)\n",
        "        return action\n",
        "\n",
        "        \n",
        "    def get_monthly_stocks(self):\n",
        "        \n",
        "        return self.monthly_stock_history\n",
        "\n",
        "    \n",
        "    def reset_monthly_history(self):\n",
        "        \n",
        "        # self.monthly_action_history = []\n",
        "        self.monthly_stock_history = []\n",
        "    def _replace_target_params():\n",
        "        pass\n",
        "      \n",
        "    def learn(self):\n",
        "        # check to replace target parameters\n",
        "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
        "            self.sess.run(self.target_replace_op)\n",
        "            print('\\ntarget_params_replaced\\n')\n",
        "\n",
        "        # sample batch memory from all memory\n",
        "        if self.memory_counter > self.memory_size:\n",
        "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
        "        else:\n",
        "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
        "        batch_memory = self.memory[sample_index, :]\n",
        "\n",
        "        _, cost = self.sess.run(\n",
        "            [self._train_op, self.loss],\n",
        "            feed_dict={\n",
        "                self.s: batch_memory[:, :self.n_features],\n",
        "                self.a: batch_memory[:, self.n_features],\n",
        "                self.r: batch_memory[:, self.n_features + 1],\n",
        "                self.s_: batch_memory[:, -self.n_features:],\n",
        "            })\n",
        "\n",
        "        self.cost_his.append(cost)\n",
        "\n",
        "        # increasing epsilon\n",
        "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
        "        self.learn_step_counter += 1\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    DQN = DeepQNetwork(2,4, output_graph=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZn0XB_A9DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trainer#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class trainer():\n",
        "    \n",
        "    def __init__(self, station_history):\n",
        "        \n",
        "        # Session Properties\n",
        "        self.episodes = []\n",
        "        self.stock_type = \"\"\n",
        "        self.logging = False\n",
        "        self.env_debug = False\n",
        "        self.rl_debug = False\n",
        "        self.inventory_station = None\n",
        "        self.operator = None\n",
        "        self.sim_stock = []\n",
        "        self.model_based = False\n",
        "        self.method = None\n",
        "        self.station_history = station_history\n",
        "        \n",
        "        # Performance Metric\n",
        "        self.success_ratio = 0\n",
        "        self.rewards = []  # [[r from session 1], [r from session 2] ...]\n",
        "        self.avg_rewards = [] #[np.mean([r from session 1]), np.mean([r from session 2])...]\n",
        "        self.final_stocks = [] # [[stock from session 1], [stock from session 2] ...]\n",
        "        self.episode_action_history = []\n",
        "        self.episode_stock_history = []\n",
        "        self.session_action_history = []\n",
        "        self.session_stock_history = []\n",
        "        self.q_tables = []\n",
        "        self.actions = [0, 5, 10, 15]\n",
        "        \n",
        "    \n",
        "    def start(self, episodes, stock_type, logging, env_debug, rl_debug, brain, model_based):\n",
        "        #DQN\n",
        "        \n",
        "        self.episodes = episodes\n",
        "        self.stock_type = stock_type\n",
        "        self.logging = logging\n",
        "        self.env_debug = env_debug\n",
        "        self.rl_debug = rl_debug\n",
        "        self.brain = brain\n",
        "        self.model_based = model_based\n",
        "       \n",
        "        self.method = 'DQN'\n",
        "        \n",
        "        idx = 0\n",
        "        \n",
        "        for eps in self.episodes:\n",
        "        \n",
        "            # Initiate new evironment and RL agent\n",
        "            self.inventory_station = env(self.stock_type, debug = self.env_debug, \n",
        "                                    station_history = self.station_history)\n",
        "            self.sim_stock.append(self.inventory_station.get_sim_stock())\n",
        "\n",
        "            if self.brain == 'dqn':\n",
        "                self.operator = DeepQNetwork(self.inventory_station.n_actions, self.inventory_station.n_features, 0.01, 0.9)\n",
        "            else:\n",
        "                print(\"Error: pick correct brain\")\n",
        "                break\n",
        "            \n",
        "            # Train the RL agent and collect performance stats\n",
        "            rewards, final_stocks = self.train_operator(idx, len(self.episodes), eps,\n",
        "            logging = self.logging, brain = self.brain, model_based = self.model_based)\n",
        "            \n",
        "            # Log the results from this training session\n",
        "            self.rewards.append(rewards)\n",
        "            self.avg_rewards.append(np.mean(rewards))\n",
        "            self.final_stocks.append(final_stocks)\n",
        "            #self.q_tables.append(self.operator.get_q_table())\n",
        "            self.session_action_history.append(self.episode_action_history)\n",
        "            self.session_stock_history.append(self.episode_stock_history)\n",
        "            self.reset_episode_history()\n",
        "            \n",
        "            # Destroy the environment and agent objects\n",
        "            self.inventory_station = None\n",
        "            self.operator = None\n",
        "            \n",
        "            idx += 1\n",
        "        \n",
        "        if logging == True:\n",
        "            self.save_session_results_dqn(self.get_timestamp(replace = True))\n",
        "            \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def train_operator(self, idx, num_sessions, episodes, logging, brain, model_based):\n",
        "    \n",
        "        '''\n",
        "        This function trains an RL agent by interacting with the inventory station \n",
        "        environment. It also tracks and reports performance stats.\n",
        "        Input:\n",
        "            - episodes: a int of episode to be trained in this session (e.g. 500)\n",
        "        Output:\n",
        "            - reward_list: a list of reward per episode in this sesison\n",
        "            - final_stocks: a list of final stocks per episode in this session\n",
        "        '''\n",
        "        \n",
        "        print(\"Start training the Agent ...\")\n",
        "        rewards = 0\n",
        "        reward_list = []\n",
        "        final_stocks = []\n",
        "        step = 0\n",
        "        \n",
        "        for eps in range(episodes):\n",
        "            \n",
        "            self.inventory_station.reset()\n",
        "                \n",
        "            while True:\n",
        "                \n",
        "                # Agent picks an action (number of inventorys to move)\n",
        "                # Agent sends the action to inventory station environment\n",
        "                # Agent gets feedback from the environment (e.g. reward of the action, new inventory stock after the action, etc.)\n",
        "                # Agent \"learn\" the feedback by updating its Q-Table (state, action, reward)\n",
        "                # Repeat until end of month (30 days)\n",
        "                # Reset inventory station environment to start a new day, repeat all\n",
        "                \n",
        "                \n",
        "               \n",
        "                  action = self.operator.choose_action(self.inventory_station.get_old_stock())\n",
        "                  current_day, old_stock, new_stock, reward, done = self.inventory_station.ping_dqn(action)\n",
        "                  self.operator.store_transition(old_stock, action, reward, new_stock)\n",
        "                  if step > 50 and (step % 10 == 0):\n",
        "                      self.operator.learn()\n",
        "\n",
        "                #observation_, reward, done = self.inventory_station.ping(action)\n",
        "            if done == True:\n",
        "                    \n",
        "                    print(\"{} of {} Session | Episode: {} | Final Stock: {} |Final Reward: {:.2f}\".format(idx, \n",
        "                          num_sessions, eps, old_stock, rewards))\n",
        "\n",
        "                    \n",
        "                    reward_list.append(rewards)\n",
        "                    final_stocks.append(old_stock)\n",
        "                    rewards = 0\n",
        "                    \n",
        "                   \n",
        "                    self.episode_stock_history.append(self.operator.get_daily_stocks());\n",
        "                    self.operator.reset_monthly_history()\n",
        "                                    \n",
        "                    break\n",
        "\n",
        "\n",
        "                    step +=1\n",
        "                    rewards += reward\n",
        "                \n",
        "                # Log monthly action history by each episode\n",
        "\n",
        "\n",
        "            with open('dqn_log.txt', 'a') as f:\n",
        "                f.write(\"{} of {} Session | Episode: {} | Final Stock: {} |Final Reward: {:.2f} \\n\".format(idx, \n",
        "                    num_sessions, eps, old_stock, rewards))\n",
        "\n",
        "                            \n",
        "        return reward_list, final_stocks\n",
        "    \n",
        "    def get_timestamp(self, replace):\n",
        "        \n",
        "        if replace == True:\n",
        "        \n",
        "            return str(datetime.datetime.now()).replace(\" \", \"\").replace(\":\", \"\").\\\n",
        "                        replace(\".\", \"\").replace(\"-\", \"\")\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            return str(datetime.datetime.now())\n",
        "    \n",
        "    \n",
        "    def reset_episode_history(self):\n",
        "        \n",
        "        self.episode_action_history = []\n",
        "        self.episode_stock_history = []\n",
        "        \n",
        "    \n",
        "    def cal_performance(self):\n",
        "        \n",
        "        successful_stocking = []\n",
        "        \n",
        "        print(\"===== Performance =====\")\n",
        "        \n",
        "        for session in range(len(self.final_stocks)):\n",
        "            length = len(self.final_stocks[session])\n",
        "            num_overstock = np.count_nonzero(np.array(self.final_stocks[session]) > 50)\n",
        "            num_understock = np.count_nonzero(np.array(self.final_stocks[session]) <= 0)\n",
        "            ratio = (length - num_understock - num_overstock)*100 / length\n",
        "            \n",
        "            print(\"Session {} | Overstock {} Times | Understock {} Times | {}% Successful\".format(session, num_overstock, \n",
        "                  num_understock, ratio))\n",
        "\n",
        "            average_reward = round(self.avg_rewards[session], 2)\n",
        "            print(\"Average Episode Reward for Session: {}\".format(average_reward))\n",
        "            \n",
        "            successful_stocking.append(ratio)\n",
        "        \n",
        "        return successful_stocking\n",
        "    \n",
        "    \n",
        "    def save_session_results(self, timestamp):\n",
        "        \n",
        "        '''\n",
        "        This function logs the following: \n",
        "            - overall success ratio of each session\n",
        "            - line chart of success ratio by session\n",
        "            - line chart of reward history by session\n",
        "            - Q Table of each session\n",
        "            - Comparison Line Chart of First and Last Episode monthly Actions\n",
        "        '''\n",
        "        \n",
        "        # --- create a session folder ---\n",
        "        dir_path = \"./performance_log/\" + timestamp\n",
        "        \n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "            \n",
        "        successful_stocking = self.cal_performance()\n",
        "        \n",
        "        # --- Write Success Rate to File ---\n",
        "        fname = dir_path + \"/success_rate - \" + timestamp + \".txt\"\n",
        "        \n",
        "        with open(fname, 'w') as f:\n",
        "            \n",
        "            f.write(\"Logged at {}\".format(self.get_timestamp(replace = False)))\n",
        "            f.write(\"\\n\")\n",
        "            f.write(\"This training session ran episodes: {}\".format(self.episodes))\n",
        "            f.write(\"\\n\")\n",
        "        \n",
        "            for session in range(len(successful_stocking)):\n",
        "                f.write(\"Session {} | Episodes: {} | Success Rate: {:.2f}%\".format(session, \n",
        "                        self.episodes[session], successful_stocking[session]))\n",
        "                f.write(\"\\n\")\n",
        "        \n",
        "        # --- Plot Overall Success Rate by Episode ---\n",
        "        \n",
        "        title = \"% of Successful Rebalancing - \" + timestamp\n",
        "        \n",
        "        fig1 = plt.figure()\n",
        "        plt.plot(self.episodes, successful_stocking)\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"% Success Rate\")\n",
        "        plt.title(title)\n",
        "        fig1.savefig(dir_path + \"/session_success_rate_\" + timestamp)\n",
        "        \n",
        "        # --- Plot Reward History by Training Session ---\n",
        "        \n",
        "        for session in range(len(self.rewards)):\n",
        "            \n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            \n",
        "            title = \"Reward History by Training Session \" + str(session) + \" - \" + timestamp\n",
        "            \n",
        "            x_axis = [x for x in range(self.episodes[session])]\n",
        "            plt.plot(x_axis, self.rewards[session], label = \"Session \"+str(session))\n",
        "            plt.legend()\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Reward\")\n",
        "            plt.title(title)\n",
        "            fig.savefig(dir_path + \"/reward_history_session_\" + \\\n",
        "                        str(session) + timestamp)\n",
        "            \n",
        "        # --- Plot Average Reward History by Training Session ---\n",
        "        figR = plt.figure(figsize=[10, 8])\n",
        "        lengths = [len(r) for r in self.rewards]\n",
        "        means = [np.mean(r) for r in self.rewards]\n",
        "        if len(self.rewards) > 1:\n",
        "            increment = (lengths[1]-lengths[0])/20\n",
        "        else:\n",
        "            increment = lengths[0]/20\n",
        "\n",
        "        for reward_list in self.rewards:\n",
        "            Q3 = np.percentile(reward_list, 75)\n",
        "            Q1 = np.percentile(reward_list, 25)\n",
        "            M = np.mean(reward_list)\n",
        "            location = len(reward_list)\n",
        "            plt.plot([location-increment, location+increment], [Q1, Q1], 'k-')\n",
        "            plt.plot([location-increment, location+increment], [Q3, Q3], 'k-')\n",
        "            plt.plot([location, location], [Q1, Q3], 'k-')\n",
        "            plt.scatter(location, M, s=100, color='dodgerblue')           \n",
        "\n",
        "        plt.xlabel('Number of Episodes in Session')\n",
        "        plt.ylabel('Average Reward per Episode')\n",
        "        plt.title('Average Reward vs. Session Size', size=20)\n",
        "        plt.xticks(lengths)\n",
        "\n",
        "        plt.plot(lengths, means, linestyle='--')\n",
        "        \n",
        "        figR.savefig(dir_path + \"/reward_averages\")\n",
        "\n",
        "        # --- Save Q tables --- \n",
        "        \n",
        "        for session in range(len(self.q_tables)):\n",
        "            \n",
        "            self.q_tables[session].to_csv(dir_path + \"/q_table_session_\" + \\\n",
        "                        str(session) + timestamp + \".csv\")\n",
        "        \n",
        "        # --- Comparison Line Chart of First and Last Episode for each Session ---\n",
        "        \n",
        "        file_path = dir_path + \"/action_history\"\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)       \n",
        "        \n",
        "        \n",
        "        for session in range(len(self.session_action_history)):\n",
        "            \n",
        "            first_eps_idx = 0\n",
        "            last_eps_idx = len(self.session_action_history[session])-1\n",
        "            \n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            title = \"Session \" + str(session) + \" - monthly Action of Eps \" + str(first_eps_idx) + \" and Eps \" + str(last_eps_idx)\n",
        "            \n",
        "            x_axis = [x for x in range(len(self.session_action_history[session][0]))]\n",
        "            plt.plot(x_axis, self.session_action_history[session][0], label = \"Eps 0\")\n",
        "            plt.plot(x_axis, self.session_action_history[session][-1], \n",
        "                     label = \"Eps \" + str(last_eps_idx))\n",
        "            \n",
        "            plt.legend()\n",
        "            plt.xlabel(\"days\")\n",
        "            plt.ylabel(\"Number of inventorys Moved\")\n",
        "            plt.title(title)\n",
        "            \n",
        "            fig.savefig(file_path + \"/action_history_\" + str(session) + timestamp)\n",
        "        \n",
        "        \n",
        "        # --- Comparison Line Chart of Simulated and Rebalanced inventory Stock --- #\n",
        "        file_path = dir_path + \"/stock_history\"\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)       \n",
        "        \n",
        "        \n",
        "        for session in range(len(self.session_stock_history)):\n",
        "            \n",
        "            first_eps_idx = 0\n",
        "            last_eps_idx = len(self.session_action_history[session])-1\n",
        "            \n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            title = \"[\" + self.method + \"]\" + \"Session \" + str(session) + \" - Original vs. Balanced inventory Stock after \" + str(first_eps_idx) + \" and Eps \" + str(last_eps_idx)\n",
        "            \n",
        "            x_axis = [x for x in range(len(self.session_stock_history[session][0]))]\n",
        "            plt.plot(x_axis, self.sim_stock[session], label = \"Original without Balancing\")\n",
        "            plt.plot(x_axis, self.session_stock_history[session][0], label = \"Balanced inventory Stock - Eps 0\")\n",
        "            plt.plot(x_axis, self.session_stock_history[session][-1], \n",
        "                     label = \"Balanced inventory Stock - Eps \" + str(last_eps_idx))\n",
        "            \n",
        "            plt.axhline(y = 50, c = \"r\", ls = \"--\", label = \"Upper Stock Limit\")\n",
        "            plt.axhline(y = 0, c = \"r\", ls = \"--\", label = \"Lower Stock Limit\")\n",
        "            \n",
        "            plt.legend()\n",
        "            plt.xlabel(\"days\")\n",
        "            plt.ylabel(\"Number of inventory Stock\")\n",
        "            plt.title(title)\n",
        "            \n",
        "            fig.savefig(file_path + \"/stock_history_\" + str(session) + timestamp)\n",
        "        \n",
        "        return\n",
        "\n",
        "    def save_session_results_dqn(self, timestamp):\n",
        "        dir_path = \"./performance_log/\" + timestamp\n",
        "        \n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "            \n",
        "        # --- Comparison Line Chart of Simulated and Rebalaned inventory Stock --- #\n",
        "        file_path = dir_path + \"/stock_history\"\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)       \n",
        "        \n",
        "        \n",
        "        successful_stocking = self.cal_performance()\n",
        "        \n",
        "        # --- Write Success Rate to File ---\n",
        "        fname = dir_path + \"/success_rate - \" + timestamp + \".txt\"\n",
        "        \n",
        "        with open(fname, 'w') as f:\n",
        "            \n",
        "            f.write(\"Logged at {}\".format(self.get_timestamp(replace = False)))\n",
        "            f.write(\"\\n\")\n",
        "            f.write(\"This training session ran episodes: {}\".format(self.episodes))\n",
        "            f.write(\"\\n\")\n",
        "        \n",
        "            for session in range(len(successful_stocking)):\n",
        "                f.write(\"Session {} | Episodes: {} | Success Rate: {:.2f}%\".format(session, \n",
        "                        self.episodes[session], successful_stocking[session]))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            # --- Plot Overall Success Rate by Episode ---\n",
        "        \n",
        "        title = \"% of Successful Rebalancing - \" + timestamp\n",
        "        \n",
        "        fig1 = plt.figure()\n",
        "        plt.plot(self.episodes, successful_stocking)\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"% Success Rate\")\n",
        "        plt.title(title)\n",
        "        fig1.savefig(dir_path + \"/session_success_rate_\" + timestamp)\n",
        "\n",
        "        for session in range(len(self.session_stock_history)):\n",
        "            \n",
        "            first_eps_idx = 0\n",
        "            last_eps_idx = len(self.session_stock_history[session])-1\n",
        "            \n",
        "            fig = plt.figure(figsize=(10, 8))\n",
        "            title = \"[\" + self.method + \"]\" + \" Session \" + str(session) + \" - Original vs. Balanced inventory Stock after \" + str(first_eps_idx) + \" and Eps \" + str(last_eps_idx)\n",
        "            \n",
        "            x_axis = [x for x in range(len(self.session_stock_history[session][0]))]\n",
        "            plt.plot(x_axis, self.sim_stock[session], label = \"Original without Balancing\")\n",
        "            plt.plot(x_axis, self.session_stock_history[session][0], label = \"Balanced inventory Stock - Eps 0\")\n",
        "            plt.plot(x_axis, self.session_stock_history[session][-1], \n",
        "                     label = \"Balanced inventory Stock - Eps \" + str(last_eps_idx))\n",
        "            \n",
        "            plt.axhline(y = 50, c = \"r\", ls = \"--\", label = \"Upper Stock Limit\")\n",
        "            plt.axhline(y = 0, c = \"r\", ls = \"--\", label = \"Lower Stock Limit\")\n",
        "            \n",
        "            plt.legend()\n",
        "            plt.xlabel(\"days\")\n",
        "            plt.ylabel(\"Number of inventory Stock\")\n",
        "            plt.title(title)\n",
        "            \n",
        "            fig.savefig(file_path + \"/stock_history_\" + \"DQN\" + str(session) + timestamp)\n",
        "        \n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssw9ESfrWoeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main#\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Get Initial Parameters    \n",
        "    episode_list, data, brain, model_based, station_history = user_input() \n",
        "    trainer_DQN = trainer(station_history)            \n",
        "    trainer_DQN.start(episode_list, data, logging  = True, env_debug = False, rl_debug = False, brain='dqn', model_based = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}